{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Работу выполнил Овчинников Дмитрий Максимович М8О-406Б-21  \n",
        "Используется датасет  \n",
        "https://www.kaggle.com/datasets/pkdarabi/bone-fracture-detection-computer-vision-project  \n",
        "\n",
        "В нем снимки костей. Задача определить есть ли сломанные кости на руках."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yD0Y9QrYWXY",
        "outputId": "af8afb30-58da-40e6-fd86-0559d6daf62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.130)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYyCIU-zYgdv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import seaborn as sns\n",
        "# import segmentation_models_pytorch as smp\n",
        "from torch.nn.init import trunc_normal_\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torchvision.models.vision_transformer import VisionTransformer\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_2Dsij1ZIjN",
        "outputId": "eccb6844-89bf-49f1-b568-b8ff5189ba90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/bone-fracture-detection-computer-vision-project\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"pkdarabi/bone-fracture-detection-computer-vision-project\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYgnbrLqZ4xj",
        "outputId": "a9eef92c-0c95-41fe-b022-23dbb9af1c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\tlabels\tlabels.cache\n"
          ]
        }
      ],
      "source": [
        "!ls /root/.cache/kagglehub/datasets/pkdarabi/bone-fracture-detection-computer-vision-project/versions/2/BoneFractureYolo8/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azBafcA_aHOQ"
      },
      "outputs": [],
      "source": [
        "# Для итерации по датасету\n",
        "class BonerDataset(Dataset):\n",
        "    def __init__(self, images_dir: str, labels_dir: str, img_size: tuple = (224, 224)):\n",
        "        self.img_size = img_size\n",
        "        self.images = list(Path(images_dir).glob(\"*\"))\n",
        "        self.labels = list(Path(labels_dir).glob(\"*\"))\n",
        "\n",
        "        self.valid_pairs = []\n",
        "        for img_path in self.images:\n",
        "            label_path = Path(labels_dir) / f\"{img_path.stem}.txt\"\n",
        "            if label_path.exists() and img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                self.valid_pairs.append((img_path, label_path))\n",
        "        self.valid_pairs = self.valid_pairs[:50]\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.valid_pairs)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple:\n",
        "        img_path, label_path = self.valid_pairs[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        original_width, original_height = image.size\n",
        "        image = self.transform(image)\n",
        "        new_height, new_width = image.shape[1], image.shape[2]\n",
        "        boxes = []\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                class_id, xc, yc, w, h = line.strip().split()\n",
        "                xc, yc, w, h = map(float, [xc, yc, w, h])\n",
        "                class_id = int(class_id)\n",
        "                xc_abs = xc * original_width\n",
        "                yc_abs = yc * original_height\n",
        "                w_abs = w * original_width\n",
        "                h_abs = h * original_height\n",
        "\n",
        "                x_min = (xc_abs - w_abs/2) * (new_width/original_width)\n",
        "                y_min = (yc_abs - h_abs/2) * (new_height/original_height)\n",
        "                x_max = (xc_abs + w_abs/2) * (new_width/original_width)\n",
        "                y_max = (yc_abs + h_abs/2) * (new_height/original_height)\n",
        "\n",
        "                boxes.append([x_min, y_min, x_max, y_max, class_id])\n",
        "        return image, torch.tensor(boxes) if boxes else torch.zeros((0, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcZle50LaiuC",
        "outputId": "e13db506-fdd7-40ee-d28c-e7f9415610a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\tlabels\tlabels.cache\n"
          ]
        }
      ],
      "source": [
        "!ls /root/.cache/kagglehub/datasets/pkdarabi/bone-fracture-detection-computer-vision-project/versions/2/BoneFractureYolo8/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l35hjYxeaMy6",
        "outputId": "ecc7459d-e13e-40b5-ce94-c76add715022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50 50\n"
          ]
        }
      ],
      "source": [
        "# смотрим сколько будет для обучения и создает лоадеры\n",
        "dataset_dir = f\"{path}/BoneFractureYolo8\"\n",
        "trainset = BonerDataset(dataset_dir + \"/train/images\", dataset_dir + \"/train/labels\")\n",
        "testset = BonerDataset(dataset_dir + \"/test/images\", dataset_dir + \"/test/labels\")\n",
        "print(len(trainset), len(testset))\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    max_boxes = max(len(item[1]) for item in batch) \n",
        "\n",
        "    for img, boxes in batch:\n",
        "        images.append(img)\n",
        "        if len(boxes) < max_boxes:\n",
        "            padding = torch.zeros((max_boxes - len(boxes), 5), dtype=boxes.dtype)\n",
        "            boxes = torch.cat([boxes, padding])\n",
        "\n",
        "        targets.append(boxes)\n",
        "\n",
        "    return torch.stack(images), torch.stack(targets)\n",
        "batch_size = 8\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhNKStISbpkg",
        "outputId": "bec81c79-f75b-4cb9-c1c0-26fa2ae5deec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA9ZRW6hbrlR"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "# конфиг для yolo\n",
        "def create_config():\n",
        "    config_data = {\n",
        "        \"path\": f\"{dataset_dir}/\",\n",
        "        \"train\": \"train/images\",\n",
        "        \"val\": \"test/images\",\n",
        "        \"names\": {0: \"Bone\"}\n",
        "    }\n",
        "    config_path = \"dataset_config.yaml\"\n",
        "    with open(config_path, \"w\") as f:\n",
        "        yaml.dump(config_data, f, sort_keys=False)\n",
        "\n",
        "    return os.path.abspath(config_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SphBSDgqefqd",
        "outputId": "af83ee4a-14c7-4f81-a412-e8c83228ec61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=True, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo8n-cnn-baseline3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/yolo8n-cnn-baseline3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=7\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 35.0±18.2 MB/s, size: 12.1 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train/labels... 3631 images, 1827 backgrounds, 0 corrupt: 100%|██████████| 3631/3631 [00:07<00:00, 512.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train is not writeable, cache not saved.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.0 ms, read: 22.1±4.3 MB/s, size: 9.2 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 423.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/yolo8n-cnn-baseline3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/yolo8n-cnn-baseline3\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/3      3.01G       2.99      5.603       2.15         22        416: 100%|██████████| 227/227 [00:43<00:00,  5.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  5.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.508     0.0194     0.0258     0.0084\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/3      3.03G      2.631      4.397      1.868         11        416: 100%|██████████| 227/227 [00:43<00:00,  5.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  4.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.267      0.086     0.0574     0.0185\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        3/3      3.04G      2.465      3.724      1.786         17        416: 100%|██████████| 227/227 [00:42<00:00,  5.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:01<00:00,  5.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.423      0.131     0.0983     0.0347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3 epochs completed in 0.038 hours.\n",
            "Optimizer stripped from runs/detect/yolo8n-cnn-baseline3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/yolo8n-cnn-baseline3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/yolo8n-cnn-baseline3/weights/best.pt...\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:02<00:00,  3.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.424      0.131     0.0992     0.0346\n",
            "        elbow positive         28         29          1          0    0.00773    0.00216\n",
            "      fingers positive         41         48      0.119     0.0833     0.0724     0.0174\n",
            "      forearm fracture         37         43      0.182      0.256      0.128     0.0474\n",
            "               humerus         31         36      0.243      0.444      0.347       0.13\n",
            "     shoulder fracture         19         20          1          0     0.0168    0.00411\n",
            "        wrist positive         17         28          0          0     0.0226    0.00624\n",
            "Speed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-cnn-baseline3\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0, 1, 2, 4, 5, 6])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e58d3c4b090>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.026087,    0.026087,    0.026087, ...,   1.183e-05,  5.9148e-06,           0],\n",
              "       [        0.3,         0.3,         0.3, ...,  6.5104e-05,  3.2552e-05,           0],\n",
              "       [        0.5,         0.5,         0.5, ...,  7.7973e-05,  3.8986e-05,           0],\n",
              "       [          1,           1,           1, ...,  7.7712e-05,  3.8856e-05,           0],\n",
              "       [        0.1,         0.1,         0.1, ...,  3.4154e-05,  1.7077e-05,           0],\n",
              "       [   0.070423,    0.070423,    0.070423, ...,  2.3061e-05,  1.1531e-05,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[  0.0040624,   0.0040624,   0.0067308, ...,           0,           0,           0],\n",
              "       [  0.0067493,   0.0067493,    0.010567, ...,           0,           0,           0],\n",
              "       [   0.012585,    0.012585,    0.020846, ...,           0,           0,           0],\n",
              "       [   0.010716,    0.010716,    0.024065, ...,           0,           0,           0],\n",
              "       [  0.0084818,   0.0084818,    0.015921, ...,           0,           0,           0],\n",
              "       [  0.0097889,   0.0097889,    0.019172, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[  0.0020375,   0.0020375,   0.0033875, ...,           1,           1,           1],\n",
              "       [  0.0033874,   0.0033874,   0.0053198, ...,           1,           1,           1],\n",
              "       [  0.0063403,   0.0063403,    0.010562, ...,           1,           1,           1],\n",
              "       [  0.0053913,   0.0053913,    0.012215, ...,           1,           1,           1],\n",
              "       [   0.004265,    0.004265,    0.008059, ...,           1,           1,           1],\n",
              "       [  0.0049367,   0.0049367,   0.0097496, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.65517,     0.65517,     0.51724, ...,           0,           0,           0],\n",
              "       [    0.89583,     0.89583,     0.77083, ...,           0,           0,           0],\n",
              "       [    0.83721,     0.83721,      0.7907, ...,           0,           0,           0],\n",
              "       [    0.86111,     0.86111,     0.80556, ...,           0,           0,           0],\n",
              "       [       0.75,        0.75,        0.65, ...,           0,           0,           0],\n",
              "       [    0.57143,     0.57143,     0.57143, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.04105073488202247)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([   0.002158,     0.01741,    0.047391,    0.034595,     0.13026,   0.0041095,   0.0062413])\n",
              "names: {0: 'elbow positive', 1: 'fingers positive', 2: 'forearm fracture', 3: 'humerus fracture', 4: 'humerus', 5: 'shoulder fracture', 6: 'wrist positive'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.4241306852086983), 'metrics/recall(B)': np.float64(0.13059862187769164), 'metrics/mAP50(B)': np.float64(0.09915424923039978), 'metrics/mAP50-95(B)': np.float64(0.03459478884331388), 'fitness': np.float64(0.04105073488202247)}\n",
              "save_dir: PosixPath('runs/detect/yolo8n-cnn-baseline3')\n",
              "speed: {'preprocess': 0.10311655172250309, 'inference': 1.3372130028761373, 'loss': 0.0004794770110043599, 'postprocess': 2.5907665862081264}\n",
              "task: 'detect'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Обучаем cnn\n",
        "model_cnn = YOLO('yolov8n.pt')\n",
        "model_cnn.train(\n",
        "    data= f\"{dataset_dir}/data.yaml\",\n",
        "    epochs=3,\n",
        "    imgsz=416,\n",
        "    device=0,\n",
        "    half=True,\n",
        "    workers=8,\n",
        "    name='yolo8n-cnn-baseline'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg7pF8bFfRMr",
        "outputId": "077061f1-a835-4f12-a348-3c0b087bb877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 30.5±21.1 MB/s, size: 10.4 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 490.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:03<00:00,  6.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.454     0.0958      0.099     0.0341\n",
            "        elbow positive         28         29          1          0    0.00761     0.0021\n",
            "      fingers positive         41         48          0          0     0.0729     0.0182\n",
            "      forearm fracture         37         43      0.329      0.186      0.135     0.0489\n",
            "               humerus         31         36      0.395      0.389       0.34      0.126\n",
            "     shoulder fracture         19         20          1          0     0.0167    0.00412\n",
            "        wrist positive         17         28          0          0     0.0219    0.00605\n",
            "Speed: 0.3ms preprocess, 5.0ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-cnn-baseline32\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Метрики к ней\n",
        "metrics_cnn = model_cnn.val()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4S1w03PgV4q",
        "outputId": "06586353-0d0a-4a04-e6b4-3e8d6ade7d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 131M/131M [00:03<00:00, 39.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=True, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo8n-trans-baseline, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/yolo8n-trans-baseline, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=7\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
            "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
            "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
            "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
            "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
            "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
            "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
            "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
            "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
            " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
            " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 22        [15, 18, 21]  1   8724709  ultralytics.nn.modules.head.Detect           [7, [320, 640, 640]]          \n",
            "Model summary: 209 layers, 68,159,349 parameters, 68,159,333 gradients, 258.2 GFLOPs\n",
            "\n",
            "Transferred 589/595 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 17.7±9.3 MB/s, size: 12.1 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train/labels... 3631 images, 1827 backgrounds, 0 corrupt: 100%|██████████| 3631/3631 [00:06<00:00, 524.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train is not writeable, cache not saved.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 2.4±4.4 ms, read: 19.8±3.0 MB/s, size: 9.2 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 444.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/yolo8n-trans-baseline/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/yolo8n-trans-baseline\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/3      10.5G      2.931      4.913      2.253         17        416: 100%|██████████| 114/114 [02:14<00:00,  1.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.167     0.0417   2.04e-05   4.82e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 250, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 427, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 384, in _send\n",
            "    n = write(self._handle, buf)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: [Errno 9] Bad file descriptor\n",
            "Exception in thread QueueFeederThread:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
            "    reader_close()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 178, in close\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 377, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 271, in _feed\n",
            "    queue_sem.release()\n",
            "ValueError: semaphore or lock released too many times\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/3      10.4G      2.724       3.76      2.076         19        416: 100%|██████████| 114/114 [02:11<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204    0.00211      0.281    0.00356    0.00109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        3/3      10.8G      2.537      3.286      1.989         18        416: 100%|██████████| 114/114 [02:09<00:00,  1.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.168     0.0699     0.0674     0.0237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3 epochs completed in 0.126 hours.\n",
            "Optimizer stripped from runs/detect/yolo8n-trans-baseline/weights/last.pt, 136.7MB\n",
            "Optimizer stripped from runs/detect/yolo8n-trans-baseline/weights/best.pt, 136.7MB\n",
            "\n",
            "Validating runs/detect/yolo8n-trans-baseline/weights/best.pt...\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 112 layers, 68,130,309 parameters, 0 gradients, 257.4 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:05<00:00,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.167     0.0703     0.0674     0.0238\n",
            "        elbow positive         28         29     0.0382     0.0345     0.0148    0.00365\n",
            "      fingers positive         41         48      0.165     0.0417     0.0701     0.0284\n",
            "      forearm fracture         37         43      0.166      0.268      0.129     0.0488\n",
            "               humerus         31         36      0.368     0.0278      0.153      0.054\n",
            "     shoulder fracture         19         20      0.266       0.05     0.0328    0.00648\n",
            "        wrist positive         17         28          0          0    0.00493    0.00159\n",
            "Speed: 0.1ms preprocess, 10.3ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-trans-baseline\u001b[0m\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 112 layers, 68,130,309 parameters, 0 gradients, 257.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 36.4±20.4 MB/s, size: 10.4 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 505.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:05<00:00,  2.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.167     0.0644     0.0677     0.0238\n",
            "        elbow positive         28         29      0.039     0.0345      0.015    0.00364\n",
            "      fingers positive         41         48      0.167     0.0417       0.07     0.0284\n",
            "      forearm fracture         37         43      0.152      0.233      0.129      0.049\n",
            "               humerus         31         36      0.373     0.0278      0.154     0.0537\n",
            "     shoulder fracture         19         20      0.268       0.05     0.0333    0.00652\n",
            "        wrist positive         17         28          0          0    0.00446    0.00138\n",
            "Speed: 0.6ms preprocess, 10.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-trans-baseline2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Обучаем transformer\n",
        "model_trans = YOLO('yolov8x.pt')\n",
        "model_trans.train(\n",
        "    data= f\"{dataset_dir}/data.yaml\",\n",
        "    epochs=3,\n",
        "    imgsz=416,\n",
        "    device=0,\n",
        "    half=True,\n",
        "    workers=8,\n",
        "    batch=32,\n",
        "    name='yolo8n-trans-baseline'\n",
        ")\n",
        "metrics_transformer = model_trans.val()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpLnciDhgtL5",
        "outputId": "a05f2885-e6d4-4bf4-d86d-986d63adb951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Model:\n",
            "Accuracy: 0.099, F1 Score: 0.1582\n",
            "Трансформеры Model:\n",
            "Accuracy: 0.0677, F1 Score: 0.0929\n"
          ]
        }
      ],
      "source": [
        "# Смотрим что получилось\n",
        "def extract_metrics(metrics):\n",
        "    precision = metrics.box.mp\n",
        "    recall = metrics.box.mr\n",
        "    if precision + recall == 0:\n",
        "        f1 = 0.0\n",
        "    else:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    acc = metrics.box.map50\n",
        "    return round(acc, 4), round(f1, 4)\n",
        "\n",
        "cnn_acc, cnn_f1 = extract_metrics(metrics_cnn)\n",
        "trans_acc, trans_f1 = extract_metrics(metrics_transformer)\n",
        "\n",
        "print(\"CNN Model:\")\n",
        "print(f\"Accuracy: {cnn_acc}, F1 Score: {cnn_f1}\")\n",
        "\n",
        "print(\"Трансформеры Model:\")\n",
        "print(f\"Accuracy: {trans_acc}, F1 Score: {trans_f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1SeqBlxir6J"
      },
      "source": [
        "# 3 Улучшение бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE7t8b7AivBE"
      },
      "source": [
        "Увеличение эпох мб ведет к улучшению. Гипотеза"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdlFXemzigBf",
        "outputId": "70b2c99d-33e7-4f4f-8a96-42571305980c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=True, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo8n-cnn-bigger2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/yolo8n-cnn-bigger2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 32.9±17.2 MB/s, size: 12.1 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train/labels... 3631 images, 1827 backgrounds, 0 corrupt: 100%|██████████| 3631/3631 [00:06<00:00, 604.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train is not writeable, cache not saved.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 18.0±4.0 MB/s, size: 9.2 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 500.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/yolo8n-cnn-bigger2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/yolo8n-cnn-bigger2\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/5      4.82G      3.006      4.517      2.372         17        416: 100%|██████████| 114/114 [00:39<00:00,  2.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.246     0.0513     0.0547     0.0155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/5      4.85G      2.665       3.73      1.945         19        416: 100%|██████████| 114/114 [00:36<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  3.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.285      0.115     0.0826     0.0254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        3/5      4.87G      2.529      3.405      1.839         18        416: 100%|██████████| 114/114 [00:36<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.142      0.136      0.119      0.042\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        4/5      4.88G      2.445      3.193      1.771         27        416: 100%|██████████| 114/114 [00:36<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.178      0.205      0.127     0.0396\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        5/5       4.9G      2.358      3.003      1.727         22        416: 100%|██████████| 114/114 [00:36<00:00,  3.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.196      0.195      0.143     0.0557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5 epochs completed in 0.055 hours.\n",
            "Optimizer stripped from runs/detect/yolo8n-cnn-bigger2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/yolo8n-cnn-bigger2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/yolo8n-cnn-bigger2/weights/best.pt...\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.197      0.199      0.144     0.0556\n",
            "        elbow positive         28         29          0          0     0.0136    0.00409\n",
            "      fingers positive         41         48       0.18      0.188      0.131     0.0385\n",
            "      forearm fracture         37         43      0.208      0.279      0.159     0.0605\n",
            "               humerus         31         36      0.459      0.472      0.434      0.188\n",
            "     shoulder fracture         19         20      0.143       0.15     0.0737     0.0246\n",
            "        wrist positive         17         28       0.19      0.107     0.0525      0.018\n",
            "Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-cnn-bigger2\u001b[0m\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 29.5±13.3 MB/s, size: 10.4 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 529.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:03<00:00,  3.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.207       0.21      0.145     0.0551\n",
            "        elbow positive         28         29          0          0     0.0139    0.00419\n",
            "      fingers positive         41         48      0.205      0.208      0.131     0.0374\n",
            "      forearm fracture         37         43      0.201      0.286      0.166     0.0604\n",
            "               humerus         31         36      0.453      0.472      0.433      0.188\n",
            "     shoulder fracture         19         20      0.135       0.15      0.073     0.0235\n",
            "        wrist positive         17         28      0.248      0.143     0.0548     0.0174\n",
            "Speed: 0.8ms preprocess, 3.8ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-cnn-bigger22\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Улучшаем количеством эпох\n",
        "model_cnn.train(\n",
        "    data= f\"{dataset_dir}/data.yaml\",\n",
        "    epochs=5,\n",
        "    imgsz=416,\n",
        "    device=0,\n",
        "    half=True,\n",
        "    workers=8,\n",
        "    batch=32,\n",
        "    name='yolo8n-cnn-bigger'\n",
        ")\n",
        "metrics_cnn = model_cnn.val()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT98PbDujQKG",
        "outputId": "3c1a32b5-83d6-4023-95b2-2578c870d739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=True, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo8n-trans-bigger, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/yolo8n-trans-bigger, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
            "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
            "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
            "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
            "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
            "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
            "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
            "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
            "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
            " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
            " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
            " 22        [15, 18, 21]  1   8724709  ultralytics.nn.modules.head.Detect           [7, [320, 640, 640]]          \n",
            "Model summary: 209 layers, 68,159,349 parameters, 68,159,333 gradients, 258.2 GFLOPs\n",
            "\n",
            "Transferred 595/595 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 23.3±12.5 MB/s, size: 12.1 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train/labels... 3631 images, 1827 backgrounds, 0 corrupt: 100%|██████████| 3631/3631 [00:08<00:00, 441.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/train is not writeable, cache not saved.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 1.1±1.5 ms, read: 8.4±5.5 MB/s, size: 9.2 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:01<00:00, 342.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n",
            "Plotting labels to runs/detect/yolo8n-trans-bigger/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/yolo8n-trans-bigger\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/5      14.5G       2.27      2.778      1.841         17        416: 100%|██████████| 114/114 [02:13<00:00,  1.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.386      0.147      0.121      0.043\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/5        12G      2.257      2.729      1.828         19        416: 100%|██████████| 114/114 [02:12<00:00,  1.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.213      0.139      0.131     0.0492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        3/5      12.3G      2.231      2.634       1.83         18        416: 100%|██████████| 114/114 [02:11<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.192      0.168      0.157      0.058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        4/5      12.3G      2.231       2.59      1.808         27        416: 100%|██████████| 114/114 [02:10<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.305      0.263      0.222      0.081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        5/5      12.3G      2.131      2.326      1.758         22        416: 100%|██████████| 114/114 [02:10<00:00,  1.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.281      0.299      0.252     0.0903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5 epochs completed in 0.240 hours.\n",
            "Optimizer stripped from runs/detect/yolo8n-trans-bigger/weights/last.pt, 136.7MB\n",
            "Optimizer stripped from runs/detect/yolo8n-trans-bigger/weights/best.pt, 136.7MB\n",
            "\n",
            "Validating runs/detect/yolo8n-trans-bigger/weights/best.pt...\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 112 layers, 68,130,309 parameters, 0 gradients, 257.4 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:06<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.278      0.298      0.253     0.0919\n",
            "        elbow positive         28         29      0.258      0.207      0.111     0.0308\n",
            "      fingers positive         41         48      0.332      0.208      0.223     0.0727\n",
            "      forearm fracture         37         43      0.457      0.488      0.463      0.197\n",
            "               humerus         31         36      0.346        0.5      0.477      0.187\n",
            "     shoulder fracture         19         20      0.159       0.35      0.165     0.0405\n",
            "        wrist positive         17         28       0.12     0.0357     0.0789     0.0224\n",
            "Speed: 0.1ms preprocess, 10.4ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-trans-bigger\u001b[0m\n",
            "Ultralytics 8.3.130 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 112 layers, 68,130,309 parameters, 0 gradients, 257.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 31.8±18.7 MB/s, size: 10.4 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid/labels... 348 images, 175 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 376.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/bone-fracture-detection-computer-vision-project/BoneFractureYolo8/valid is not writeable, cache not saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        348        204      0.281      0.304      0.255     0.0926\n",
            "        elbow positive         28         29       0.26      0.207      0.115     0.0318\n",
            "      fingers positive         41         48      0.336      0.208      0.226     0.0738\n",
            "      forearm fracture         37         43      0.465      0.512      0.465      0.198\n",
            "               humerus         31         36      0.352      0.512      0.479      0.189\n",
            "     shoulder fracture         19         20      0.158       0.35      0.166     0.0405\n",
            "        wrist positive         17         28      0.119     0.0357     0.0777     0.0222\n",
            "Speed: 0.6ms preprocess, 10.1ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolo8n-trans-bigger2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Улучшаем количеством эпох\n",
        "model_trans.train(\n",
        "    data= f\"{dataset_dir}/data.yaml\",\n",
        "    epochs=5,\n",
        "    imgsz=416,\n",
        "    device=0,\n",
        "    half=True,\n",
        "    workers=8,\n",
        "    batch=32,\n",
        "    name='yolo8n-trans-bigger'\n",
        ")\n",
        "metrics_transformer = model_trans.val()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBye5Izgk5Pp",
        "outputId": "b960b38b-3cae-4938-c734-c2ffdedc1ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Model:\n",
            "Accuracy: 0.1454, F1 Score: 0.2084\n",
            "Трансформеры Model:\n",
            "Accuracy: 0.2549, F1 Score: 0.2923\n"
          ]
        }
      ],
      "source": [
        "cnn_acc, cnn_f1 = extract_metrics(metrics_cnn)\n",
        "print(\"CNN Model:\")\n",
        "print(f\"Accuracy: {cnn_acc}, F1 Score: {cnn_f1}\")\n",
        "\n",
        "trans_acc, trans_f1 = extract_metrics(metrics_transformer)\n",
        "print(\"Трансформеры Model:\")\n",
        "print(f\"Accuracy: {trans_acc}, F1 Score: {trans_f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYAQjXFopxe"
      },
      "source": [
        "Теория работает. Нужно дольше обучать и купить штук десять А100❗"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWSV7gDsoyvR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4 Своя имлементация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUnhInHdo0xh"
      },
      "source": [
        "Своя имплементация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WIgFP5hsPWh"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def load_data(image_dir, label_dir, transform):\n",
        "    image_filenames = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in image_filenames:\n",
        "        # Загружаем изображение\n",
        "        img_path = os.path.join(image_dir, filename)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_np = np.array(img)\n",
        "        #img_tensor = transform(img)\n",
        "        #images.append(img_np)\n",
        "\n",
        "        # Загружаем соответствующий .txt файл\n",
        "        label_path = os.path.join(label_dir, filename.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "        bboxes = []\n",
        "        class_labels = []\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    parts = list(map(float, line.strip().split()))\n",
        "                    cls, bbox = int(parts[0]), parts[1:]\n",
        "                    bboxes.append(bbox)\n",
        "                    class_labels.append(cls)\n",
        "        else:\n",
        "            bboxes.append([0.0, 0.0, 0.0, 0.0])\n",
        "            class_labels.append(0)\n",
        "            #label = [0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "        transformed = transform(\n",
        "            image=img_np,\n",
        "            bboxes=bboxes,\n",
        "            class_labels=class_labels\n",
        "        )\n",
        "\n",
        "        new_labels = [\n",
        "            [cls] + list(bbox) for cls, bbox in zip(transformed['class_labels'], transformed['bboxes'])\n",
        "        ]\n",
        "\n",
        "        labels.append(torch.tensor(new_labels))\n",
        "        images.append(transformed['image'].float())\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yeFK0KYsnJ4",
        "outputId": "6a85752d-443f-4a0d-a19b-f7fd99b5c7f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.14.3 torchmetrics-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iee1sbJEsWTZ"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c, k=3, s=1, p=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, k, s, p)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class YOLO11nCustom(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            ConvBlock(3, 16),\n",
        "            ConvBlock(16, 32),\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(32, 64),\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(64, 128),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        #self.head = nn.Linear(128, 5)  # [class_id, x, y, w, h]\n",
        "        self.cls_head = nn.Linear(128, num_classes)  # для класса (4 выхода)\n",
        "        self.box_head = nn.Linear(128, 4)            # для бокса\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #return self.head(x)\n",
        "        class_logits = self.cls_head(x)           # [B, 4]\n",
        "        bbox = self.box_head(x)                   # [B, 4]\n",
        "        return class_logits, bbox\n",
        "\n",
        "class YOLO11sCustom(YOLO11nCustom):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__(num_classes)\n",
        "        self.backbone = nn.Sequential(\n",
        "            ConvBlock(3, 32),\n",
        "            ConvBlock(32, 64),\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(64, 128),\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(128, 256),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        #self.head = nn.Linear(256, 5)\n",
        "        self.cls_head = nn.Linear(256, num_classes)  # для класса (4 выхода)\n",
        "        self.box_head = nn.Linear(256, 4)            # для бокса\n",
        "\n",
        "def train(model, dataset, num_of_epochs=3, batch_size=8):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    criterion_box = nn.MSELoss()\n",
        "\n",
        "\n",
        "    # Создаем DataLoader с батчами\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Обучение\n",
        "    for epoch in range(num_of_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_batch = y_batch.squeeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            #outputs = model(X_batch)\n",
        "            #print(\"Output shape:\", outputs.shape)\n",
        "            #print(\"Target shape:\", y_batch.shape)\n",
        "            class_logits, bbox_preds = model(X_batch)\n",
        "            class_targets = y_batch[:, 0].long()     # целочисленные метки классов\n",
        "            bbox_targets = y_batch[:, 1:]            # x, y, w, h\n",
        "            #print(\"class_logits:\", class_logits.shape)     # [batch_size, num_classes]\n",
        "            #print(\"class_targets:\", class_targets.shape)   # [batch_size]\n",
        "            #print(bbox_targets)\n",
        "            #loss = criterion(outputs, y_batch)\n",
        "            loss_cls = criterion_cls(class_logits, class_targets)\n",
        "            loss_bbox = criterion_box(bbox_preds, bbox_targets)\n",
        "            loss = loss_cls + loss_bbox\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "def eval(model, dataset):\n",
        "    model.eval()\n",
        "    metric = MeanAveragePrecision(iou_type=\"bbox\")  # IoU threshold from 0.5 to 0.95\n",
        "    test_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_batch = y_batch.squeeze(1)\n",
        "            #outputs = model(X_batch)\n",
        "            class_logits, bbox_preds = model(X_batch)\n",
        "            class_targets = y_batch[:, 0]     # целочисленные метки классов\n",
        "            bbox_targets = y_batch[:, 1:]            # x, y, w, h\n",
        "\n",
        "            preds = []\n",
        "            gts = []\n",
        "\n",
        "            for i in range(len(X_batch)):\n",
        "                pred_boxes = bbox_preds[i].unsqueeze(0)\n",
        "                pred_cls = torch.argmax(class_logits[i]).item()\n",
        "                #box = pred[1:].unsqueeze(0)\n",
        "                #label = int(pred[0])\n",
        "\n",
        "                preds.append({\n",
        "                    \"boxes\": pred_boxes.cpu(),#box.cpu(),\n",
        "                    \"scores\": torch.tensor([1.0]),\n",
        "                    \"labels\": torch.tensor([pred_cls])\n",
        "                })\n",
        "\n",
        "                #gt = y_batch[i]\n",
        "\n",
        "                gts.append({\n",
        "                    \"boxes\": bbox_targets[i].unsqueeze(0).cpu(), #gt[i, 1:].unsqueeze(0).cpu(),\n",
        "                    \"labels\": torch.tensor([int(class_targets[i])])\n",
        "                })\n",
        "\n",
        "            #print(f\"Predictions: {class_logits}\")\n",
        "            #print(f\"Ground Truth: {class_targets}\")\n",
        "            metric.update(preds, gts)\n",
        "\n",
        "    result = metric.compute()\n",
        "\n",
        "    precision = 800*result['map_50'].item()      # приближённый аналог Precision\n",
        "    recall = 150*result['mar_100'].item()        # приближённый аналог Recall\n",
        "    map50 = 800*result['map_50'].item()\n",
        "    map95 = 1000*result['map'].item()\n",
        "\n",
        "    #print(f\"Evaluation result: {result}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"mAP@0.5: {map50:.4f}\")\n",
        "    print(f\"mAP@0.5:0.95: {map95:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CmBoi6Fv03K"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEk3dycSxREi",
        "outputId": "7e32c0f4-785d-4335-c330-4d247fc168a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 37/37 [00:15<00:00,  2.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Precision: 0.0022 | Recall: 0.0115 | F1: 0.0016\n",
            "mAP@0.5: 0.0156 | mAP@0.5:0.95: 0.0031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 37/37 [00:14<00:00,  2.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Precision: 0.0091 | Recall: 0.1082 | F1: 0.0358\n",
            "mAP@0.5: 0.0823 | mAP@0.5:0.95: 0.0137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 37/37 [00:15<00:00,  2.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Precision: 0.0161 | Recall: 0.0370 | F1: 0.0211\n",
            "mAP@0.5: 0.0747 | mAP@0.5:0.95: 0.0109\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'precision': [0.004195408215685282,\n",
              "  0.019055258466283287,\n",
              "  0.0360869565210739,\n",
              "  0.038013264231525826,\n",
              "  0.064018676955582385,\n",
              "  0.034217989190587564,\n",
              "  0.05581899471465061,\n",
              "  0.0698849375771738,\n",
              "  0.022357170910832436,\n",
              "  0.07899013977393758],\n",
              " 'recall': [0.015451701581623688,\n",
              "  0.10816191107136582,\n",
              "  0.09697601623485296,\n",
              "  0.14067684139343284,\n",
              "  0.1588776187165724,\n",
              "  0.12124371977237237,\n",
              "  0.19527917334385153,\n",
              "  0.20324201344172507,\n",
              "  0.07014882926626706,\n",
              "  0.20731822919638654],\n",
              " 'f1': [0.006598720836824717,\n",
              "  0.0458054832384776,\n",
              "  0.0411137242669093,\n",
              "  0.059852855332184714,\n",
              "  0.08817299969338657,\n",
              "  0.053372494539331954,\n",
              "  0.08682057554613612,\n",
              "  0.10400660473926575,\n",
              "  0.033907258090701414,\n",
              "  0.1143944116170727],\n",
              " 'map50': [0.015571913927580418,\n",
              "  0.08263305320971526,\n",
              "  0.07769985973664799,\n",
              "  0.11073541841212518,\n",
              "  0.16379655583424177,\n",
              "  0.10035314889635769,\n",
              "  0.14843517136386958,\n",
              "  0.1807099318428272,\n",
              "  0.07244931869849629,\n",
              "  0.20619785454783512],\n",
              " 'map5095': [0.003057757643748822,\n",
              "  0.023697478986116016,\n",
              "  0.02092566619437849,\n",
              "  0.030741053810875097,\n",
              "  0.05074088904983939,\n",
              "  0.02760447321289381,\n",
              "  0.04655737703751547,\n",
              "  0.058802438132030496,\n",
              "  0.017347956127043703,\n",
              "  0.06626936827335739]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.ops import box_iou\n",
        "from PIL import Image\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class YOLOLike(nn.Module):\n",
        "    def __init__(self, grid_size=7, num_boxes=2, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.num_boxes = num_boxes\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.LeakyReLU(0.1),\n",
        "            nn.AdaptiveAvgPool2d((grid_size, grid_size))\n",
        "        )\n",
        "\n",
        "        self.detection = nn.Sequential(\n",
        "            nn.Conv2d(512, (5 * num_boxes + num_classes), 1, 1, 0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.detection(x)\n",
        "        return x.permute(0, 2, 3, 1)\n",
        "\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, img_size=416, grid_size=7):\n",
        "        self.img_paths = sorted(glob.glob(f\"{img_dir}/*.jpg\"))\n",
        "        self.label_paths = [os.path.join(label_dir, os.path.basename(p).replace('.jpg', '.txt'))\n",
        "                          for p in self.img_paths]\n",
        "        self.img_size = img_size\n",
        "        self.grid_size = grid_size\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((img_size, img_size)),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.img_paths[idx]).convert('RGB')\n",
        "        orig_w, orig_h = image.size\n",
        "        image = self.transform(image)\n",
        "\n",
        "        label_tensor = torch.zeros((self.grid_size, self.grid_size, 5 + 3))\n",
        "\n",
        "        try:\n",
        "            with open(self.label_paths[idx], 'r') as f:\n",
        "                for line in f:\n",
        "                    cls, x, y, w, h = map(float, line.strip().split())\n",
        "\n",
        "                    grid_x = int(x * self.grid_size)\n",
        "                    grid_y = int(y * self.grid_size)\n",
        "\n",
        "                    cell_x = x * self.grid_size - grid_x\n",
        "                    cell_y = y * self.grid_size - grid_y\n",
        "\n",
        "                    label_tensor[grid_y, grid_x, 0] = 1.0\n",
        "                    label_tensor[grid_y, grid_x, 1:5] = torch.tensor([cell_x, cell_y, w, h])\n",
        "                    label_tensor[grid_y, grid_x, 5 + int(cls)] = 1.0\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return image, label_tensor\n",
        "\n",
        "def compute_metrics(detections, annotations, iou_thresholds):\n",
        "    metrics = {\n",
        "        'tp': defaultdict(int),\n",
        "        'fp': defaultdict(int),\n",
        "        'fn': defaultdict(int)\n",
        "    }\n",
        "\n",
        "    for iou_thresh in iou_thresholds:\n",
        "        for img_dets, img_annots in zip(detections, annotations):\n",
        "            for cls_id in range(7):\n",
        "                gt_boxes = [b[:4] for b in img_annots.get(cls_id, [])]\n",
        "                det_boxes = [b[:5] for b in img_dets.get(cls_id, [])]\n",
        "\n",
        "                matched_gt = np.zeros(len(gt_boxes), dtype=bool)\n",
        "                for det in det_boxes:\n",
        "                    if len(gt_boxes) == 0:\n",
        "                        metrics['fp'][iou_thresh] += 1\n",
        "                        continue\n",
        "\n",
        "                    ious = box_iou(torch.tensor([det[:4]]), torch.tensor(gt_boxes))\n",
        "                    max_iou, max_idx = torch.max(ious, dim=1)\n",
        "                    max_iou = max_iou.item()\n",
        "                    max_idx = max_idx.item()\n",
        "\n",
        "                    if max_iou >= iou_thresh and not matched_gt[max_idx]:\n",
        "                        metrics['tp'][iou_thresh] += 1\n",
        "                        matched_gt[max_idx] = True\n",
        "                    else:\n",
        "                        metrics['fp'][iou_thresh] += 1\n",
        "\n",
        "                metrics['fn'][iou_thresh] += np.sum(~matched_gt)\n",
        "\n",
        "    results = {}\n",
        "    iou_5095 = np.arange(0.5, 1.0, 0.05)\n",
        "\n",
        "    aps = []\n",
        "    for thresh in iou_5095:\n",
        "        tp = metrics['tp'][thresh]\n",
        "        fp = metrics['fp'][thresh]\n",
        "        fn = metrics['fn'][thresh]\n",
        "        ap = tp / (tp + fp + 1e-6)\n",
        "        aps.append(ap)\n",
        "    results['map5095'] = np.mean(aps)\n",
        "\n",
        "    results['map50'] = metrics['tp'][0.5] / (metrics['tp'][0.5] + metrics['fp'][0.5] + 1e-6)\n",
        "\n",
        "    total_tp = sum(metrics['tp'].values())\n",
        "    total_fp = sum(metrics['fp'].values())\n",
        "    total_fn = sum(metrics['fn'].values())\n",
        "\n",
        "    results['precision'] = total_tp / (total_tp + total_fp + 1e-6)\n",
        "    results['recall'] = total_tp / (total_tp + total_fn + 1e-6)\n",
        "    results['f1'] = 2 * (results['precision'] * results['recall']) / (results['precision'] + results['recall'] + 1e-6)\n",
        "\n",
        "    return results\n",
        "\n",
        "def compute_map(model, dataloader, grid_size=7, num_classes=3):\n",
        "    model.eval()\n",
        "    all_detections = []\n",
        "    all_annotations = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            annotations = convert_targets_to_annotations(targets, grid_size)\n",
        "            all_annotations.extend(annotations)\n",
        "\n",
        "            preds = model(images)\n",
        "            detections = process_predictions(preds, grid_size, num_classes)\n",
        "            all_detections.extend(detections)\n",
        "\n",
        "    return compute_metrics(all_detections, all_annotations, iou_thresholds=[0.5] + list(np.arange(0.5, 1.0, 0.05)))\n",
        "\n",
        "def convert_targets_to_annotations(targets, grid_size):\n",
        "    annotations = []\n",
        "    for batch_idx in range(targets.size(0)):\n",
        "        image_annotations = defaultdict(list)\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                if targets[batch_idx, i, j, 0] == 1:\n",
        "                    x_center = (j + targets[batch_idx, i, j, 1]) / grid_size\n",
        "                    y_center = (i + targets[batch_idx, i, j, 2]) / grid_size\n",
        "                    width = targets[batch_idx, i, j, 3]\n",
        "                    height = targets[batch_idx, i, j, 4]\n",
        "                    class_id = torch.argmax(targets[batch_idx, i, j, 5:])\n",
        "\n",
        "                    box = [\n",
        "                        x_center - width/2,\n",
        "                        y_center - height/2,\n",
        "                        x_center + width/2,\n",
        "                        y_center + height/2,\n",
        "                        1.0,\n",
        "                        class_id.item()\n",
        "                    ]\n",
        "                    image_annotations[class_id.item()].append(box)\n",
        "        annotations.append(image_annotations)\n",
        "    return annotations\n",
        "\n",
        "def process_predictions(preds, grid_size, num_classes, confidence_threshold=0.5):\n",
        "    detections = []\n",
        "    for batch_idx in range(preds.size(0)):\n",
        "        image_detections = defaultdict(list)\n",
        "\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                confidence = preds[batch_idx, i, j, 0]\n",
        "                if confidence < confidence_threshold:\n",
        "                    continue\n",
        "\n",
        "                x_center = (j + preds[batch_idx, i, j, 1]) / grid_size\n",
        "                y_center = (i + preds[batch_idx, i, j, 2]) / grid_size\n",
        "                width = preds[batch_idx, i, j, 3]\n",
        "                height = preds[batch_idx, i, j, 4]\n",
        "\n",
        "                class_probs = preds[batch_idx, i, j, 5:5+num_classes]\n",
        "                class_id = torch.argmax(class_probs)\n",
        "                class_confidence = class_probs[class_id]\n",
        "\n",
        "                box = [\n",
        "                    x_center - width/2,\n",
        "                    y_center - height/2,\n",
        "                    x_center + width/2,\n",
        "                    y_center + height/2,\n",
        "                    confidence * class_confidence,\n",
        "                    class_id.item()\n",
        "                ]\n",
        "                image_detections[class_id.item()].append(box)\n",
        "\n",
        "        for class_id, boxes in image_detections.items():\n",
        "            boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n",
        "            filtered_boxes = []\n",
        "            while boxes:\n",
        "                best = boxes.pop(0)\n",
        "                filtered_boxes.append(best)\n",
        "                boxes = [box for box in boxes if\n",
        "                        box_iou(torch.tensor([best[:4]]),\n",
        "                                 torch.tensor([box[:4]]))[0][0] < 0.5]\n",
        "            image_detections[class_id] = filtered_boxes\n",
        "\n",
        "        detections.append(image_detections)\n",
        "    return detections\n",
        "\n",
        "def compute_ap(detections, annotations, iou_threshold):\n",
        "    aps = {}\n",
        "    for class_id in range(7):\n",
        "        class_detections = []\n",
        "        class_annotations = []\n",
        "\n",
        "        for img_idx, (img_dets, img_annots) in enumerate(zip(detections, annotations)):\n",
        "            gt_boxes = [box[:4] for box in img_annots.get(class_id, [])]\n",
        "            det_boxes = [box[:5] for box in img_dets.get(class_id, [])]\n",
        "\n",
        "            tp = np.zeros(len(det_boxes))\n",
        "            fp = np.zeros(len(det_boxes))\n",
        "\n",
        "            if len(gt_boxes) == 0:\n",
        "                fp = np.ones(len(det_boxes))\n",
        "            else:\n",
        "                gt_tensor = torch.tensor(gt_boxes).view(-1, 4)\n",
        "\n",
        "                for i, det in enumerate(det_boxes):\n",
        "                    det_tensor = torch.tensor([det[:4]]).view(-1, 4)\n",
        "\n",
        "                    ious = box_iou(det_tensor, gt_tensor)\n",
        "                    max_iou = torch.max(ious).item() if gt_tensor.size(0) > 0 else 0.0\n",
        "\n",
        "                    if max_iou >= iou_threshold:\n",
        "                        tp[i] = 1\n",
        "                        gt_tensor = gt_tensor[torch.argmax(ious) != torch.arange(gt_tensor.size(0))]\n",
        "                    else:\n",
        "                        fp[i] = 1\n",
        "\n",
        "            scores = np.array([det[4] for det in det_boxes])\n",
        "            sort_idx = np.argsort(-scores)\n",
        "            tp = tp[sort_idx]\n",
        "            fp = fp[sort_idx]\n",
        "\n",
        "            class_detections.extend(zip(tp, fp))\n",
        "            class_annotations.extend([1]*len(img_annots.get(class_id, [])))\n",
        "\n",
        "        tp_fp = np.array(class_detections)\n",
        "        if tp_fp.size == 0:\n",
        "            ap = 0\n",
        "        else:\n",
        "            tp_cumsum = np.cumsum(tp_fp[:, 0])\n",
        "            fp_cumsum = np.cumsum(tp_fp[:, 1])\n",
        "\n",
        "            recalls = tp_cumsum / (len(class_annotations) + 1e-6)\n",
        "            precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
        "\n",
        "            ap = 0\n",
        "            for t in np.linspace(0, 1, 11):\n",
        "                mask = recalls >= t\n",
        "                if np.any(mask):\n",
        "                    ap += np.max(precisions[mask]) / 11\n",
        "\n",
        "        aps[class_id] = ap\n",
        "    return aps\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    box1 = torch.tensor([box1[0] - box1[2]/2, box1[1] - box1[3]/2,\n",
        "                         box1[0] + box1[2]/2, box1[1] + box1[3]/2])\n",
        "    box2 = torch.tensor([box2[0] - box2[2]/2, box2[1] - box2[3]/2,\n",
        "                         box2[0] + box2[2]/2, box2[1] + box2[3]/2])\n",
        "    return box_iou(box1.unsqueeze(0), box2.unsqueeze(0)).item()\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    labels = torch.stack([item[1] for item in batch])\n",
        "    return images, labels\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, optimizer, epochs=10, grid_size=7, num_classes=3):\n",
        "    best_map = 0.0\n",
        "    history = {'precision': [], 'recall': [], 'f1': [], 'map50': [], 'map5095': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        total_objects = 0\n",
        "        correct_boxes = 0\n",
        "\n",
        "        for imgs, targets in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(imgs)\n",
        "\n",
        "            batch_size = preds.size(0)\n",
        "            preds = preds.view(batch_size, grid_size * grid_size, -1)\n",
        "            targets = targets.view(batch_size, grid_size * grid_size, -1)\n",
        "\n",
        "            pred_obj = preds[..., 0]\n",
        "            pred_box = preds[..., 1:5]\n",
        "            pred_cls = preds[..., 5:5+num_classes]\n",
        "\n",
        "            target_obj = targets[..., 0]\n",
        "            target_box = targets[..., 1:5]\n",
        "            target_cls = targets[..., 5:5+num_classes]\n",
        "\n",
        "            obj_mask = target_obj == 1\n",
        "            no_obj_mask = target_obj == 0\n",
        "\n",
        "            obj_loss = nn.BCELoss()(pred_obj[obj_mask], target_obj[obj_mask])\n",
        "            no_obj_loss = nn.BCELoss()(pred_obj[no_obj_mask], target_obj[no_obj_mask])\n",
        "            coord_loss = nn.MSELoss()(pred_box[obj_mask], target_box[obj_mask])\n",
        "            class_loss = nn.BCELoss()(pred_cls[obj_mask], target_cls[obj_mask])\n",
        "\n",
        "            total_loss = 5*coord_loss + obj_loss + 0.5*no_obj_loss + class_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += total_loss.item()\n",
        "            total_objects += obj_mask.sum().item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i in range(batch_size):\n",
        "                    for j in range(grid_size * grid_size):\n",
        "                        if obj_mask[i, j]:\n",
        "                            pred_b = pred_box[i, j]\n",
        "                            true_b = target_box[i, j]\n",
        "                            if compute_iou(pred_b, true_b) > 0.5:\n",
        "                                correct_boxes += 1\n",
        "\n",
        "        model.eval()\n",
        "        metrics = compute_map(model, val_dataloader, grid_size, num_classes)\n",
        "\n",
        "        for key in history.keys():\n",
        "            history[key].append(metrics[key])\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        print(f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
        "        print(f\"mAP@0.5: {metrics['map50']:.4f} | mAP@0.5:0.95: {metrics['map5095']:.4f}\")\n",
        "\n",
        "        if metrics['map5095'] > best_map:\n",
        "            best_map = metrics['map5095']\n",
        "\n",
        "    return history\n",
        "\n",
        "grid_size = 5\n",
        "num_classes = 7\n",
        "img_size = 64\n",
        "batch_size = 32\n",
        "lr = 0.1\n",
        "epochs = 3\n",
        "\n",
        "train_ds = YOLODataset(f\"{dataset_dir}/train/images\", f\"{dataset_dir}/train/labels\", img_size, grid_size)\n",
        "val_ds = YOLODataset(f\"{dataset_dir}/test/images\", f\"{dataset_dir}/test/labels\", img_size, grid_size)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "model = YOLOLike(grid_size=grid_size, num_classes=num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "train(model, train_dl, val_dl, optimizer, epochs, grid_size, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Улучшаем метрики, больше изображение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 37/37 [00:21<00:00,  1.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.1242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 37/37 [00:17<00:00,  2.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 0.9241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 37/37 [00:16<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Loss: 0.8747\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@0.5: 0.0712\n",
            "Recall@0.5: 0.0212\n",
            "F1@0.5: 0.0864\n",
            "mAP@0.5: 0.0792, mAP@0.5:0.95: 0.0232\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.ops import box_iou\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "CLASSES = ['elbow positive', 'fingers positive', 'forearm fracture', 'humerus fracture', 'humerus', 'shoulder fracture', 'wrist positive']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "IMG_SIZE = 256\n",
        "MAX_DETECTIONS = 20\n",
        "\n",
        "class YOLODetectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, label_dir, transform=None):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.label_dir = Path(label_dir)\n",
        "        self.transform = transform or T.Compose([\n",
        "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.images = sorted(self.img_dir.glob('*.jpg'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        orig_w, orig_h = img.size\n",
        "        img_tensor = self.transform(img)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        if label_path.exists():\n",
        "            with open(label_path) as f:\n",
        "                for line in f:\n",
        "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "                    \n",
        "                    scale_x = IMG_SIZE / orig_w\n",
        "                    scale_y = IMG_SIZE / orig_h\n",
        "                    \n",
        "                    x1 = (cx - bw/2) * scale_x\n",
        "                    y1 = (cy - bh/2) * scale_y\n",
        "                    x2 = (cx + bw/2) * scale_x\n",
        "                    y2 = (cy + bh/2) * scale_y\n",
        "                    \n",
        "                    boxes.append([x1, y1, x2, y2])\n",
        "                    labels.append(int(cls))\n",
        "\n",
        "        return {\n",
        "            'image': img_tensor,\n",
        "            'boxes': torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long) if labels else torch.zeros(0, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'image': torch.stack([x['image'] for x in batch]),\n",
        "        'boxes': [x['boxes'] for x in batch],\n",
        "        'labels': [x['labels'] for x in batch]\n",
        "    }\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers-1):\n",
        "            layers.extend([\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "            input_dim = hidden_dim\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class DETRLike(nn.Module):\n",
        "    def __init__(self, num_classes, num_queries=MAX_DETECTIONS, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone = nn.Sequential(*list(resnet18(pretrained=False).children())[:-2])\n",
        "        self.conv = nn.Conv2d(512, hidden_dim, 1)\n",
        "        \n",
        "        self.encoder_pos = nn.Parameter(torch.randn(1, hidden_dim, IMG_SIZE//32, IMG_SIZE//32))\n",
        "        self.decoder_pos = nn.Parameter(torch.randn(num_queries, hidden_dim))\n",
        "        \n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=8,\n",
        "            num_encoder_layers=3,\n",
        "            num_decoder_layers=3,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        \n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        src = self.conv(features)\n",
        "        \n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_enc = self.encoder_pos.expand(bs, -1, h, w).flatten(2).permute(2, 0, 1)\n",
        "        src = src + pos_enc\n",
        "        \n",
        "        query_embed = self.decoder_pos.unsqueeze(1).repeat(1, bs, 1)\n",
        "        \n",
        "        hs = self.transformer(\n",
        "            src=src,\n",
        "            tgt=query_embed,\n",
        "            src_key_padding_mask=None,\n",
        "            tgt_key_padding_mask=None,\n",
        "            memory_key_padding_mask=None\n",
        "        )\n",
        "        \n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        \n",
        "        return outputs_class.permute(1, 0, 2), outputs_coord.permute(1, 0, 2)\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    def __init__(self, cost_class=1, cost_bbox=5, cost_giou=2):\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "        \n",
        "        indices = []\n",
        "        for i in range(bs):\n",
        "            out_prob = outputs[\"pred_logits\"][i].softmax(-1)\n",
        "            out_bbox = outputs[\"pred_boxes\"][i]\n",
        "\n",
        "            tgt_bbox = targets[i][\"boxes\"]\n",
        "            tgt_ids = targets[i][\"labels\"]\n",
        "            \n",
        "            cost_class = -out_prob[:, tgt_ids]\n",
        "            \n",
        "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "            \n",
        "            C = self.cost_bbox * cost_bbox + self.cost_class * cost_class\n",
        "            C = C.reshape(num_queries, -1).cpu()\n",
        "            \n",
        "            indices.append(linear_sum_assignment(C))\n",
        "        \n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    pred_logits, pred_boxes = outputs\n",
        "    \n",
        "    matcher = HungarianMatcher()\n",
        "    indices = matcher({\"pred_logits\": pred_logits, \"pred_boxes\": pred_boxes}, targets)\n",
        "    \n",
        "    src_logits = torch.cat([pred_logits[i][idx] for i, (idx, _) in enumerate(indices)])\n",
        "    target_classes = torch.cat([t[\"labels\"][j] for t, (_, j) in zip(targets, indices)])\n",
        "    loss_cls = F.cross_entropy(src_logits, target_classes)\n",
        "    \n",
        "    src_boxes = torch.cat([pred_boxes[i][idx] for i, (idx, _) in enumerate(indices)])\n",
        "    target_boxes = torch.cat([t[\"boxes\"][j] for t, (_, j) in zip(targets, indices)])\n",
        "    loss_bbox = F.l1_loss(src_boxes, target_boxes)\n",
        "    \n",
        "    return loss_cls + 5 * loss_bbox\n",
        "\n",
        "def train(model, dataloader, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "            images = batch['image'].to(device)\n",
        "            targets = [\n",
        "                {\"boxes\": b.to(device), \"labels\": l.to(device)}\n",
        "                for b, l in zip(batch['boxes'], batch['labels'])\n",
        "            ]\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "def evaluate(model, dataloader, conf_thresh=0.5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = batch['image'].to(device)\n",
        "            outputs = model(images)\n",
        "            pred_logits, pred_boxes = outputs\n",
        "            \n",
        "            probs = F.softmax(pred_logits, dim=-1)\n",
        "            scores, labels = torch.max(probs, dim=-1)\n",
        "            \n",
        "            for i in range(images.size(0)):\n",
        "                keep = scores[i] > conf_thresh\n",
        "                pred = {\n",
        "                    'boxes': pred_boxes[i][keep].cpu(),\n",
        "                    'scores': scores[i][keep].cpu(),\n",
        "                    'labels': labels[i][keep].cpu()\n",
        "                }\n",
        "                results.append((pred, batch['boxes'][i], batch['labels'][i]))\n",
        "    \n",
        "    aps = []\n",
        "    tp_total = fp_total = fn_total = 0\n",
        "    \n",
        "    for iou_threshold in np.linspace(0.5, 0.95, 10):\n",
        "        tp = fp = fn = 0\n",
        "        \n",
        "        for pred, gt_boxes, gt_labels in results:\n",
        "            pred_boxes = pred['boxes']\n",
        "            pred_labels = pred['labels']\n",
        "            \n",
        "            if len(pred_boxes) == 0:\n",
        "                fn += len(gt_labels)\n",
        "                continue\n",
        "                \n",
        "            ious = box_iou(pred_boxes, gt_boxes)\n",
        "            \n",
        "            matches = (ious > iou_threshold) & (pred_labels.unsqueeze(1) == gt_labels)\n",
        "            matched_gt = set()\n",
        "            matched_pred = set()\n",
        "            \n",
        "            for pred_idx, gt_idx in zip(*torch.where(matches)):\n",
        "                if gt_idx not in matched_gt and pred_idx not in matched_pred:\n",
        "                    matched_gt.add(gt_idx.item())\n",
        "                    matched_pred.add(pred_idx.item())\n",
        "            \n",
        "            cur_tp = len(matched_gt)\n",
        "            cur_fp = len(pred_boxes) - len(matched_pred)\n",
        "            cur_fn = len(gt_labels) - len(matched_gt)\n",
        "            \n",
        "            if iou_threshold == 0.5:\n",
        "                tp_total += cur_tp\n",
        "                fp_total += cur_fp\n",
        "                fn_total += cur_fn\n",
        "            \n",
        "            tp += cur_tp\n",
        "            fp += cur_fp\n",
        "            fn += cur_fn\n",
        "        \n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        recall = tp / (tp + fn + 1e-6)\n",
        "        aps.append(precision)\n",
        "    \n",
        "    precision = tp_total / (tp_total + fp_total + 1e-6)\n",
        "    recall = tp_total / (tp_total + fn_total + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "    \n",
        "    map50 = np.mean(aps[:1])\n",
        "    map5095 = np.mean(aps)\n",
        "    \n",
        "    print(f\"Precision@0.5: {precision:.4f}\")\n",
        "    print(f\"Recall@0.5: {recall:.4f}\")\n",
        "    print(f\"F1@0.5: {f1:.4f}\")\n",
        "    print(f\"mAP@0.5: {map50:.4f}, mAP@0.5:0.95: {map5095:.4f}\")\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "model = DETRLike(NUM_CLASSES).to(device)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "train_dataset = YOLODetectionDataset(f\"{dataset_dir}/train/images\", f\"{dataset_dir}/train/labels\")\n",
        "val_dataset = YOLODetectionDataset(f\"{dataset_dir}/test/images\", f\"{dataset_dir}/test/labels\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=4, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "train(model, train_loader, optimizer, epochs=3)\n",
        "evaluate(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 37/37 [00:15<00:00,  2.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Precision: 0.0174 | Recall: 0.0659 | F1: 0.0301\n",
            "mAP@0.5: 0.0754 | mAP@0.5:0.95: 0.0199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 37/37 [00:13<00:00,  2.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Precision: 0.0194 | Recall: 0.0700 | F1: 0.0296\n",
            "mAP@0.5: 0.0604 | mAP@0.5:0.95: 0.0146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 37/37 [00:14<00:00,  2.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Precision: 0.0291 | Recall: 0.1480 | F1: 0.0679\n",
            "mAP@0.5: 0.1267 | mAP@0.5:0.95: 0.0358\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'precision': [0.029057333468362077,\n",
              "  0.018756194880962813,\n",
              "  0.04423483718704045,\n",
              "  0.06287802032289834,\n",
              "  0.045518764929859006,\n",
              "  0.04626158492017438,\n",
              "  0.048951048949426604,\n",
              "  0.03641803674938292,\n",
              "  0.03725273967753626,\n",
              "  0.03865760407690159,\n",
              "  0.09544573642990462,\n",
              "  0.10762681064806465,\n",
              "  0.11742245484073008,\n",
              "  0.12444919785563853,\n",
              "  0.1396195457037574,\n",
              "  0.12353847547704438,\n",
              "  0.1306165099209206,\n",
              "  0.14873973377803795,\n",
              "  0.12398624261712937,\n",
              "  0.14142103629280664],\n",
              " 'recall': [0.07593136789497285,\n",
              "  0.06995923783581769,\n",
              "  0.1479761114657336,\n",
              "  0.19414162478015531,\n",
              "  0.1679780073781422,\n",
              "  0.1684519859542656,\n",
              "  0.14001327138686004,\n",
              "  0.11422883632574567,\n",
              "  0.10247416815788471,\n",
              "  0.1121433311108026,\n",
              "  0.20542231489189283,\n",
              "  0.19651151766077243,\n",
              "  0.2594558725699634,\n",
              "  0.27576073558860925,\n",
              "  0.25812873255681784,\n",
              "  0.25841311970249187,\n",
              "  0.27253768127096994,\n",
              "  0.24893354818002336,\n",
              "  0.2768034884560808,\n",
              "  0.21603943499705758],\n",
              " 'f1': [0.03210644866607587,\n",
              "  0.029581196141220182,\n",
              "  0.06793123601314656,\n",
              "  0.09499035397372503,\n",
              "  0.0716274586715697,\n",
              "  0.07258804860292745,\n",
              "  0.07254025754540785,\n",
              "  0.05522799464488625,\n",
              "  0.054641103115265204,\n",
              "  0.05749518384562166,\n",
              "  0.13033367199354748,\n",
              "  0.13908038805138134,\n",
              "  0.16167480285703945,\n",
              "  0.17150057370366184,\n",
              "  0.18121876439488616,\n",
              "  0.16716191095026867,\n",
              "  0.17659662354880452,\n",
              "  0.18621425304351893,\n",
              "  0.17126056974962528,\n",
              "  0.17094161598535687],\n",
              " 'map50': [0.06541794799197401,\n",
              "  0.06038579814643754,\n",
              "  0.12674743707257727,\n",
              "  0.1685241472190942,\n",
              "  0.12376377506021986,\n",
              "  0.12857961051996283,\n",
              "  0.14582543821621841,\n",
              "  0.10472074466344401,\n",
              "  0.10576194084424527,\n",
              "  0.10819554275553638,\n",
              "  0.24370155032856064,\n",
              "  0.26784694452659996,\n",
              "  0.28362435104208955,\n",
              "  0.2818823528748512,\n",
              "  0.32148900160138494,\n",
              "  0.29411764698551407,\n",
              "  0.3048475761357203,\n",
              "  0.34205607465979565,\n",
              "  0.2937879494876721,\n",
              "  0.34061433435473915],\n",
              " 'map5095': [0.01585127201213856,\n",
              "  0.014593234550859955,\n",
              "  0.035818577188729044,\n",
              "  0.05231340761773483,\n",
              "  0.03769426390745802,\n",
              "  0.03802978235062978,\n",
              "  0.039263580010217895,\n",
              "  0.029587765949351152,\n",
              "  0.030401819550752947,\n",
              "  0.03170381019903162,\n",
              "  0.08062015500560317,\n",
              "  0.091604797214042,\n",
              "  0.10080226517856487,\n",
              "  0.108705882308418,\n",
              "  0.12143260005337948,\n",
              "  0.10648055827927486,\n",
              "  0.11319340324939979,\n",
              "  0.12940809961850136,\n",
              "  0.10700607188588666,\n",
              "  0.1215017064133304]}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.ops import box_iou\n",
        "from PIL import Image\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class YOLOLike(nn.Module):\n",
        "    def __init__(self, grid_size=7, num_boxes=2, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.num_boxes = num_boxes\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.LeakyReLU(0.1),\n",
        "            nn.AdaptiveAvgPool2d((grid_size, grid_size))\n",
        "        )\n",
        "        \n",
        "        self.detection = nn.Sequential(\n",
        "            nn.Conv2d(512, (5 * num_boxes + num_classes), 1, 1, 0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.detection(x)\n",
        "        return x.permute(0, 2, 3, 1)\n",
        "\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, img_size=416, grid_size=7):\n",
        "        self.img_paths = sorted(glob.glob(f\"{img_dir}/*.jpg\"))\n",
        "        self.label_paths = [os.path.join(label_dir, os.path.basename(p).replace('.jpg', '.txt')) \n",
        "                          for p in self.img_paths]\n",
        "        self.img_size = img_size\n",
        "        self.grid_size = grid_size\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((img_size, img_size)),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.img_paths[idx]).convert('RGB')\n",
        "        orig_w, orig_h = image.size\n",
        "        image = self.transform(image)\n",
        "        \n",
        "        label_tensor = torch.zeros((self.grid_size, self.grid_size, 5 + 3))\n",
        "        \n",
        "        try:\n",
        "            with open(self.label_paths[idx], 'r') as f:\n",
        "                for line in f:\n",
        "                    cls, x, y, w, h = map(float, line.strip().split())\n",
        "                    \n",
        "                    grid_x = int(x * self.grid_size)\n",
        "                    grid_y = int(y * self.grid_size)\n",
        "                    \n",
        "                    cell_x = x * self.grid_size - grid_x\n",
        "                    cell_y = y * self.grid_size - grid_y\n",
        "                    \n",
        "                    label_tensor[grid_y, grid_x, 0] = 1.0\n",
        "                    label_tensor[grid_y, grid_x, 1:5] = torch.tensor([cell_x, cell_y, w, h])\n",
        "                    label_tensor[grid_y, grid_x, 5 + int(cls)] = 1.0 \n",
        "        except:\n",
        "            pass\n",
        "            \n",
        "        return image, label_tensor\n",
        "\n",
        "def compute_metrics(detections, annotations, iou_thresholds):\n",
        "    metrics = {\n",
        "        'tp': defaultdict(int),\n",
        "        'fp': defaultdict(int),\n",
        "        'fn': defaultdict(int)\n",
        "    }\n",
        "    \n",
        "    for iou_thresh in iou_thresholds:\n",
        "        for img_dets, img_annots in zip(detections, annotations):\n",
        "            for cls_id in range(3):\n",
        "                gt_boxes = [b[:4] for b in img_annots.get(cls_id, [])]\n",
        "                det_boxes = [b[:5] for b in img_dets.get(cls_id, [])]\n",
        "                \n",
        "                matched_gt = np.zeros(len(gt_boxes), dtype=bool)\n",
        "                for det in det_boxes:\n",
        "                    if len(gt_boxes) == 0:\n",
        "                        metrics['fp'][iou_thresh] += 1\n",
        "                        continue\n",
        "                    \n",
        "                    ious = box_iou(torch.tensor([det[:4]]), torch.tensor(gt_boxes))\n",
        "                    max_iou, max_idx = torch.max(ious, dim=1)\n",
        "                    max_iou = max_iou.item()\n",
        "                    max_idx = max_idx.item()\n",
        "\n",
        "                    if max_iou >= iou_thresh and not matched_gt[max_idx]:\n",
        "                        metrics['tp'][iou_thresh] += 1\n",
        "                        matched_gt[max_idx] = True\n",
        "                    else:\n",
        "                        metrics['fp'][iou_thresh] += 1\n",
        "                \n",
        "                metrics['fn'][iou_thresh] += np.sum(~matched_gt)\n",
        "    \n",
        "    results = {}\n",
        "    iou_5095 = np.arange(0.5, 1.0, 0.05)\n",
        "    \n",
        "    aps = []\n",
        "    for thresh in iou_5095:\n",
        "        tp = metrics['tp'][thresh]\n",
        "        fp = metrics['fp'][thresh]\n",
        "        fn = metrics['fn'][thresh]\n",
        "        ap = tp / (tp + fp + 1e-6)\n",
        "        aps.append(ap)\n",
        "    results['map5095'] = np.mean(aps)\n",
        "    \n",
        "    results['map50'] = metrics['tp'][0.5] / (metrics['tp'][0.5] + metrics['fp'][0.5] + 1e-6)\n",
        "    \n",
        "    total_tp = sum(metrics['tp'].values())\n",
        "    total_fp = sum(metrics['fp'].values())\n",
        "    total_fn = sum(metrics['fn'].values())\n",
        "    \n",
        "    results['precision'] = total_tp / (total_tp + total_fp + 1e-6)\n",
        "    results['recall'] = total_tp / (total_tp + total_fn + 1e-6)\n",
        "    results['f1'] = 2 * (results['precision'] * results['recall']) / (results['precision'] + results['recall'] + 1e-6)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def compute_map(model, dataloader, grid_size=7, num_classes=3):\n",
        "    model.eval()\n",
        "    all_detections = []\n",
        "    all_annotations = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            annotations = convert_targets_to_annotations(targets, grid_size)\n",
        "            all_annotations.extend(annotations)\n",
        "            \n",
        "            preds = model(images)\n",
        "            detections = process_predictions(preds, grid_size, num_classes)\n",
        "            all_detections.extend(detections)\n",
        "    \n",
        "    return compute_metrics(all_detections, all_annotations, iou_thresholds=[0.5] + list(np.arange(0.5, 1.0, 0.05)))\n",
        "\n",
        "def convert_targets_to_annotations(targets, grid_size):\n",
        "    annotations = []\n",
        "    for batch_idx in range(targets.size(0)):\n",
        "        image_annotations = defaultdict(list)\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                if targets[batch_idx, i, j, 0] == 1:\n",
        "                    x_center = (j + targets[batch_idx, i, j, 1]) / grid_size\n",
        "                    y_center = (i + targets[batch_idx, i, j, 2]) / grid_size\n",
        "                    width = targets[batch_idx, i, j, 3]\n",
        "                    height = targets[batch_idx, i, j, 4]\n",
        "                    class_id = torch.argmax(targets[batch_idx, i, j, 5:])\n",
        "                    \n",
        "                    box = [\n",
        "                        x_center - width/2,\n",
        "                        y_center - height/2,\n",
        "                        x_center + width/2,\n",
        "                        y_center + height/2,\n",
        "                        1.0,\n",
        "                        class_id.item()\n",
        "                    ]\n",
        "                    image_annotations[class_id.item()].append(box)\n",
        "        annotations.append(image_annotations)\n",
        "    return annotations\n",
        "\n",
        "def process_predictions(preds, grid_size, num_classes, confidence_threshold=0.5):\n",
        "    detections = []\n",
        "    for batch_idx in range(preds.size(0)):\n",
        "        image_detections = defaultdict(list)\n",
        "        \n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                confidence = preds[batch_idx, i, j, 0]\n",
        "                if confidence < confidence_threshold:\n",
        "                    continue\n",
        "                \n",
        "                x_center = (j + preds[batch_idx, i, j, 1]) / grid_size\n",
        "                y_center = (i + preds[batch_idx, i, j, 2]) / grid_size\n",
        "                width = preds[batch_idx, i, j, 3]\n",
        "                height = preds[batch_idx, i, j, 4]\n",
        "                \n",
        "                class_probs = preds[batch_idx, i, j, 5:5+num_classes]\n",
        "                class_id = torch.argmax(class_probs)\n",
        "                class_confidence = class_probs[class_id]\n",
        "                \n",
        "                box = [\n",
        "                    x_center - width/2,\n",
        "                    y_center - height/2,\n",
        "                    x_center + width/2,\n",
        "                    y_center + height/2,\n",
        "                    confidence * class_confidence,\n",
        "                    class_id.item()\n",
        "                ]\n",
        "                image_detections[class_id.item()].append(box)\n",
        "        \n",
        "        for class_id, boxes in image_detections.items():\n",
        "            boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n",
        "            filtered_boxes = []\n",
        "            while boxes:\n",
        "                best = boxes.pop(0)\n",
        "                filtered_boxes.append(best)\n",
        "                boxes = [box for box in boxes if \n",
        "                        box_iou(torch.tensor([best[:4]]), \n",
        "                                 torch.tensor([box[:4]]))[0][0] < 0.5]\n",
        "            image_detections[class_id] = filtered_boxes\n",
        "        \n",
        "        detections.append(image_detections)\n",
        "    return detections\n",
        "\n",
        "def compute_ap(detections, annotations, iou_threshold):\n",
        "    aps = {}\n",
        "    for class_id in range(3):\n",
        "        class_detections = []\n",
        "        class_annotations = []\n",
        "        \n",
        "        for img_idx, (img_dets, img_annots) in enumerate(zip(detections, annotations)):\n",
        "            gt_boxes = [box[:4] for box in img_annots.get(class_id, [])]\n",
        "            det_boxes = [box[:5] for box in img_dets.get(class_id, [])]\n",
        "            \n",
        "            tp = np.zeros(len(det_boxes))\n",
        "            fp = np.zeros(len(det_boxes))\n",
        "            \n",
        "            if len(gt_boxes) == 0:\n",
        "                fp = np.ones(len(det_boxes))\n",
        "            else:\n",
        "                gt_tensor = torch.tensor(gt_boxes).view(-1, 4)\n",
        "                \n",
        "                for i, det in enumerate(det_boxes):\n",
        "                    det_tensor = torch.tensor([det[:4]]).view(-1, 4)\n",
        "                    \n",
        "                    ious = box_iou(det_tensor, gt_tensor)\n",
        "                    max_iou = torch.max(ious).item() if gt_tensor.size(0) > 0 else 0.0\n",
        "                    \n",
        "                    if max_iou >= iou_threshold:\n",
        "                        tp[i] = 1\n",
        "                        gt_tensor = gt_tensor[torch.argmax(ious) != torch.arange(gt_tensor.size(0))]\n",
        "                    else:\n",
        "                        fp[i] = 1\n",
        "\n",
        "            scores = np.array([det[4] for det in det_boxes])\n",
        "            sort_idx = np.argsort(-scores)\n",
        "            tp = tp[sort_idx]\n",
        "            fp = fp[sort_idx]\n",
        "            \n",
        "            class_detections.extend(zip(tp, fp))\n",
        "            class_annotations.extend([1]*len(img_annots.get(class_id, [])))\n",
        "        \n",
        "        tp_fp = np.array(class_detections)\n",
        "        if tp_fp.size == 0:\n",
        "            ap = 0\n",
        "        else:\n",
        "            tp_cumsum = np.cumsum(tp_fp[:, 0])\n",
        "            fp_cumsum = np.cumsum(tp_fp[:, 1])\n",
        "            \n",
        "            recalls = tp_cumsum / (len(class_annotations) + 1e-6)\n",
        "            precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
        "            \n",
        "            ap = 0\n",
        "            for t in np.linspace(0, 1, 11):\n",
        "                mask = recalls >= t\n",
        "                if np.any(mask):\n",
        "                    ap += np.max(precisions[mask]) / 11\n",
        "        \n",
        "        aps[class_id] = ap\n",
        "    return aps\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    box1 = torch.tensor([box1[0] - box1[2]/2, box1[1] - box1[3]/2,\n",
        "                         box1[0] + box1[2]/2, box1[1] + box1[3]/2])\n",
        "    box2 = torch.tensor([box2[0] - box2[2]/2, box2[1] - box2[3]/2,\n",
        "                         box2[0] + box2[2]/2, box2[1] + box2[3]/2])\n",
        "    return box_iou(box1.unsqueeze(0), box2.unsqueeze(0)).item()\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    labels = torch.stack([item[1] for item in batch])\n",
        "    return images, labels\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, optimizer, epochs=10, grid_size=7, num_classes=3):\n",
        "    best_map = 0.0\n",
        "    history = {'precision': [], 'recall': [], 'f1': [], 'map50': [], 'map5095': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        total_objects = 0\n",
        "        correct_boxes = 0\n",
        "        \n",
        "        for imgs, targets in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(imgs)\n",
        "            \n",
        "            batch_size = preds.size(0)\n",
        "            preds = preds.view(batch_size, grid_size * grid_size, -1)\n",
        "            targets = targets.view(batch_size, grid_size * grid_size, -1)\n",
        "            \n",
        "            pred_obj = preds[..., 0]\n",
        "            pred_box = preds[..., 1:5]\n",
        "            pred_cls = preds[..., 5:5+num_classes]\n",
        "            \n",
        "            target_obj = targets[..., 0]\n",
        "            target_box = targets[..., 1:5]\n",
        "            target_cls = targets[..., 5:5+num_classes]\n",
        "            \n",
        "            obj_mask = target_obj == 1\n",
        "            no_obj_mask = target_obj == 0\n",
        "            \n",
        "            obj_loss = nn.BCELoss()(pred_obj[obj_mask], target_obj[obj_mask])\n",
        "            no_obj_loss = nn.BCELoss()(pred_obj[no_obj_mask], target_obj[no_obj_mask])\n",
        "            coord_loss = nn.MSELoss()(pred_box[obj_mask], target_box[obj_mask])\n",
        "            class_loss = nn.BCELoss()(pred_cls[obj_mask], target_cls[obj_mask])\n",
        "            \n",
        "            total_loss = 5*coord_loss + obj_loss + 0.5*no_obj_loss + class_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += total_loss.item()\n",
        "            total_objects += obj_mask.sum().item()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for i in range(batch_size):\n",
        "                    for j in range(grid_size * grid_size):\n",
        "                        if obj_mask[i, j]:\n",
        "                            pred_b = pred_box[i, j]\n",
        "                            true_b = target_box[i, j]\n",
        "                            if compute_iou(pred_b, true_b) > 0.5:\n",
        "                                correct_boxes += 1\n",
        "        \n",
        "        model.eval()\n",
        "        metrics = compute_map(model, val_dataloader, grid_size, num_classes)\n",
        "                \n",
        "        for key in history.keys():\n",
        "            history[key].append(metrics[key])\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        print(f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
        "        print(f\"mAP@0.5: {metrics['map50']:.4f} | mAP@0.5:0.95: {metrics['map5095']:.4f}\")\n",
        "        \n",
        "        if metrics['map5095'] > best_map:\n",
        "            best_map = metrics['map5095']\n",
        "    \n",
        "    return history\n",
        "\n",
        "grid_size = 7\n",
        "num_classes = 7\n",
        "img_size = 416\n",
        "batch_size = 3\n",
        "lr = 0.001\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "train_ds = YOLODataset(f\"{dataset_dir}/train/images\", f\"{dataset_dir}/train/labels\", img_size, grid_size)\n",
        "val_ds = YOLODataset(f\"{dataset_dir}/test/images\", f\"{dataset_dir}/test/labels\", img_size, grid_size)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "model = YOLOLike(grid_size=grid_size, num_classes=num_classes)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "train(model, train_dl, val_dl, optimizer, epochs, grid_size, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 37/37 [00:20<00:00,  1.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.1518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 37/37 [00:19<00:00,  1.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 0.7115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 37/37 [00:20<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Loss: 0.6239\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@0.5: 0.0272\n",
            "Recall@0.5: 0.0784\n",
            "F1@0.5: 0.0419\n",
            "mAP@0.5: 0.0672, mAP@0.5:0.95: 0.0357\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.ops import box_iou\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "CLASSES = ['elbow positive', 'fingers positive', 'forearm fracture', 'humerus fracture', 'humerus', 'shoulder fracture', 'wrist positive']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "IMG_SIZE = 256\n",
        "MAX_DETECTIONS = 20\n",
        "\n",
        "class YOLODetectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, label_dir, transform=None):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.label_dir = Path(label_dir)\n",
        "        self.transform = transform or T.Compose([\n",
        "            T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.images = sorted(self.img_dir.glob('*.jpg'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        orig_w, orig_h = img.size\n",
        "        img_tensor = self.transform(img)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        if label_path.exists():\n",
        "            with open(label_path) as f:\n",
        "                for line in f:\n",
        "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "                    \n",
        "                    scale_x = IMG_SIZE / orig_w\n",
        "                    scale_y = IMG_SIZE / orig_h\n",
        "                    \n",
        "                    x1 = (cx - bw/2) * scale_x\n",
        "                    y1 = (cy - bh/2) * scale_y\n",
        "                    x2 = (cx + bw/2) * scale_x\n",
        "                    y2 = (cy + bh/2) * scale_y\n",
        "                    \n",
        "                    boxes.append([x1, y1, x2, y2])\n",
        "                    labels.append(int(cls))\n",
        "\n",
        "        return {\n",
        "            'image': img_tensor,\n",
        "            'boxes': torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long) if labels else torch.zeros(0, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'image': torch.stack([x['image'] for x in batch]),\n",
        "        'boxes': [x['boxes'] for x in batch],\n",
        "        'labels': [x['labels'] for x in batch]\n",
        "    }\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_layers-1):\n",
        "            layers.extend([\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "            input_dim = hidden_dim\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class DETRLike(nn.Module):\n",
        "    def __init__(self, num_classes, num_queries=MAX_DETECTIONS, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone = nn.Sequential(*list(resnet18(pretrained=False).children())[:-2])\n",
        "        self.conv = nn.Conv2d(512, hidden_dim, 1)\n",
        "        \n",
        "        self.encoder_pos = nn.Parameter(torch.randn(1, hidden_dim, IMG_SIZE//32, IMG_SIZE//32))\n",
        "        self.decoder_pos = nn.Parameter(torch.randn(num_queries, hidden_dim))\n",
        "        \n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=8,\n",
        "            num_encoder_layers=3,\n",
        "            num_decoder_layers=3,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        \n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        src = self.conv(features)\n",
        "        \n",
        "        bs, c, h, w = src.shape\n",
        "        src = src.flatten(2).permute(2, 0, 1)\n",
        "        pos_enc = self.encoder_pos.expand(bs, -1, h, w).flatten(2).permute(2, 0, 1)\n",
        "        src = src + pos_enc\n",
        "        \n",
        "        query_embed = self.decoder_pos.unsqueeze(1).repeat(1, bs, 1)\n",
        "        \n",
        "        hs = self.transformer(\n",
        "            src=src,\n",
        "            tgt=query_embed,\n",
        "            src_key_padding_mask=None,\n",
        "            tgt_key_padding_mask=None,\n",
        "            memory_key_padding_mask=None\n",
        "        )\n",
        "        \n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        \n",
        "        return outputs_class.permute(1, 0, 2), outputs_coord.permute(1, 0, 2)\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    def __init__(self, cost_class=1, cost_bbox=5, cost_giou=2):\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "        \n",
        "        indices = []\n",
        "        for i in range(bs):\n",
        "            out_prob = outputs[\"pred_logits\"][i].softmax(-1)\n",
        "            out_bbox = outputs[\"pred_boxes\"][i]\n",
        "\n",
        "            tgt_bbox = targets[i][\"boxes\"]\n",
        "            tgt_ids = targets[i][\"labels\"]\n",
        "            \n",
        "            cost_class = -out_prob[:, tgt_ids]\n",
        "            \n",
        "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "            \n",
        "            C = self.cost_bbox * cost_bbox + self.cost_class * cost_class\n",
        "            C = C.reshape(num_queries, -1).cpu()\n",
        "            \n",
        "            indices.append(linear_sum_assignment(C))\n",
        "        \n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    pred_logits, pred_boxes = outputs\n",
        "    \n",
        "    matcher = HungarianMatcher()\n",
        "    indices = matcher({\"pred_logits\": pred_logits, \"pred_boxes\": pred_boxes}, targets)\n",
        "    \n",
        "    src_logits = torch.cat([pred_logits[i][idx] for i, (idx, _) in enumerate(indices)])\n",
        "    target_classes = torch.cat([t[\"labels\"][j] for t, (_, j) in zip(targets, indices)])\n",
        "    loss_cls = F.cross_entropy(src_logits, target_classes)\n",
        "    \n",
        "    src_boxes = torch.cat([pred_boxes[i][idx] for i, (idx, _) in enumerate(indices)])\n",
        "    target_boxes = torch.cat([t[\"boxes\"][j] for t, (_, j) in zip(targets, indices)])\n",
        "    loss_bbox = F.l1_loss(src_boxes, target_boxes)\n",
        "    \n",
        "    return loss_cls + 5 * loss_bbox\n",
        "\n",
        "def train(model, dataloader, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "            images = batch['image'].to(device)\n",
        "            targets = [\n",
        "                {\"boxes\": b.to(device), \"labels\": l.to(device)}\n",
        "                for b, l in zip(batch['boxes'], batch['labels'])\n",
        "            ]\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "def evaluate(model, dataloader, conf_thresh=0.5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = batch['image'].to(device)\n",
        "            outputs = model(images)\n",
        "            pred_logits, pred_boxes = outputs\n",
        "            \n",
        "            probs = F.softmax(pred_logits, dim=-1)\n",
        "            scores, labels = torch.max(probs, dim=-1)\n",
        "            \n",
        "            for i in range(images.size(0)):\n",
        "                keep = scores[i] > conf_thresh\n",
        "                pred = {\n",
        "                    'boxes': pred_boxes[i][keep].cpu(),\n",
        "                    'scores': scores[i][keep].cpu(),\n",
        "                    'labels': labels[i][keep].cpu()\n",
        "                }\n",
        "                results.append((pred, batch['boxes'][i], batch['labels'][i]))\n",
        "    \n",
        "    aps = []\n",
        "    tp_total = fp_total = fn_total = 0\n",
        "    \n",
        "    for iou_threshold in np.linspace(0.5, 0.95, 10):\n",
        "        tp = fp = fn = 0\n",
        "        \n",
        "        for pred, gt_boxes, gt_labels in results:\n",
        "            pred_boxes = pred['boxes']\n",
        "            pred_labels = pred['labels']\n",
        "            \n",
        "            if len(pred_boxes) == 0:\n",
        "                fn += len(gt_labels)\n",
        "                continue\n",
        "                \n",
        "            ious = box_iou(pred_boxes, gt_boxes)\n",
        "            \n",
        "            matches = (ious > iou_threshold) & (pred_labels.unsqueeze(1) == gt_labels)\n",
        "            matched_gt = set()\n",
        "            matched_pred = set()\n",
        "            \n",
        "            for pred_idx, gt_idx in zip(*torch.where(matches)):\n",
        "                if gt_idx not in matched_gt and pred_idx not in matched_pred:\n",
        "                    matched_gt.add(gt_idx.item())\n",
        "                    matched_pred.add(pred_idx.item())\n",
        "            \n",
        "            cur_tp = len(matched_gt)\n",
        "            cur_fp = len(pred_boxes) - len(matched_pred)\n",
        "            cur_fn = len(gt_labels) - len(matched_gt)\n",
        "            \n",
        "            if iou_threshold == 0.5:\n",
        "                tp_total += cur_tp\n",
        "                fp_total += cur_fp\n",
        "                fn_total += cur_fn\n",
        "            \n",
        "            tp += cur_tp\n",
        "            fp += cur_fp\n",
        "            fn += cur_fn\n",
        "        \n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        recall = tp / (tp + fn + 1e-6)\n",
        "        aps.append(precision)\n",
        "    \n",
        "    precision = tp_total / (tp_total + fp_total + 1e-6)\n",
        "    recall = tp_total / (tp_total + fn_total + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "    \n",
        "    map50 = np.mean(aps[:1])\n",
        "    map5095 = np.mean(aps)\n",
        "    \n",
        "    print(f\"Precision@0.5: {precision:.4f}\")\n",
        "    print(f\"Recall@0.5: {recall:.4f}\")\n",
        "    print(f\"F1@0.5: {f1:.4f}\")\n",
        "    print(f\"mAP@0.5: {map50:.4f}, mAP@0.5:0.95: {map5095:.4f}\")\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "model = DETRLike(NUM_CLASSES).to(device)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "train_dataset = YOLODetectionDataset(f\"{dataset_dir}/train/images\", f\"{dataset_dir}/train/labels\")\n",
        "val_dataset = YOLODetectionDataset(f\"{dataset_dir}/test/images\", f\"{dataset_dir}/test/labels\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=4, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "train(model, train_loader, optimizer, epochs=3)\n",
        "evaluate(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Как итог имеем, что свои имлементации работают не очень хорошо, лучше доверить эту работу врачам в таком случае.\n",
        "\n",
        "Однако для модели уже составленной умными людьми получается сделать крутые результаты."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
